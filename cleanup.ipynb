{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Cleanup\n",
    "### *Preparing raw data for modelling*\n",
    "\n",
    "> #### by [Sebastian Einar RÃ¸kholt](https://www.linkedin.com/in/sebastianrokholt/)\n",
    ">-------------------------------------------\n",
    "> *January 2021*\n",
    "----------------------------------------------------------------------------\n",
    "<a id=\"top\"></a> \n",
    "![Hybrid Recommender System Graphic](project-graphic.png)\n",
    "\n",
    "\n",
    "### Project Index:\n",
    "[GitHub Repo](http://url) <br>\n",
    "*Part 1:* **cleanup.ipynb**<br>\n",
    "*Part 2:* [analysis-and-modelling.ipynb](http://url)<br>\n",
    "*Part 3:* [predict.ipynb](http://url)<br>\n",
    "    \n",
    "### Cleanup Notebook Index:\n",
    "1. [**Cleaning 'rangering.dat': User ratings for movies**](#ratings) <br>\n",
    "2. [**Cleaning 'bruker.json': Users data**](#users) <br>\n",
    "3. [**Cleaning 'film.xlsx': Movies data**](#movies) <br>\n",
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ratings\"></a>1) Cleaning 'rangering.dat'-file: Preparing the user ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing packages for data cleanup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from math import ceil\n",
    "from os import path, makedirs\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrukerID</th>\n",
       "      <th>FilmID</th>\n",
       "      <th>Rangering</th>\n",
       "      <th>Tidstempel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900187.000000</td>\n",
       "      <td>900187.000000</td>\n",
       "      <td>900187.000000</td>\n",
       "      <td>8.986950e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2991.864495</td>\n",
       "      <td>1989.675878</td>\n",
       "      <td>4.279477</td>\n",
       "      <td>9.722414e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1736.204837</td>\n",
       "      <td>1126.366532</td>\n",
       "      <td>1.971075</td>\n",
       "      <td>1.214672e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.567039e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.653029e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2967.000000</td>\n",
       "      <td>1959.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.729904e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4501.000000</td>\n",
       "      <td>2963.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.752202e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6040.000000</td>\n",
       "      <td>3952.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.046455e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BrukerID         FilmID      Rangering    Tidstempel\n",
       "count  900187.000000  900187.000000  900187.000000  8.986950e+05\n",
       "mean     2991.864495    1989.675878       4.279477  9.722414e+08\n",
       "std      1736.204837    1126.366532       1.971075  1.214672e+07\n",
       "min         0.000000       0.000000       1.000000  9.567039e+08\n",
       "25%      1458.000000    1037.000000       3.000000  9.653029e+08\n",
       "50%      2967.000000    1959.000000       4.000000  9.729904e+08\n",
       "75%      4501.000000    2963.000000       5.000000  9.752202e+08\n",
       "max      6040.000000    3952.000000      10.000000  1.046455e+09"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset as .csv, setting column labels\n",
    "ratings_df = pd.read_csv('raw_data/rangering.dat', sep='::', header=0,\n",
    "                         names=['BrukerID', 'FilmID', 'Rangering', 'Tidstempel'],\n",
    "                         engine='python')\n",
    "\n",
    "ratings_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrukerID         0\n",
      "FilmID           0\n",
      "Rangering        0\n",
      "Tidstempel    1492\n",
      "dtype: int64 \n",
      "\n",
      "There are 900187 rows in the dataset.\n",
      "Proportion of missing data for each column in %: \n",
      "BrukerID      0.00\n",
      "FilmID        0.00\n",
      "Rangering     0.00\n",
      "Tidstempel    0.17\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_vals = ratings_df.isnull().sum()\n",
    "print(missing_vals, '\\n')\n",
    "perc = round(missing_vals / ratings_df.shape[0] * 100, 2)\n",
    "print(f'There are {ratings_df.shape[0]} rows in the dataset.')\n",
    "print(f'Proportion of missing data for each column in %: \\n{perc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost 1500 rows with missing timestamp values, which culminates to only 0.17% of the dataset. Even though 0.17% missing data is low, I don't want to simply delete these rows, as it would decrease the sample size. This would  result in a significant decrease in prediction accuracy for a large number of users. I should therefore try to fill these missing values by imputing an average. However, it would not make sense to impute the average of all timestamps, as this would tamper with calculations of how long a user has been active on the platform. A better solution is to impute the average timestamp for each individual user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_of_conversion 965088000.0\n",
      "Total number of missing values after imputing the avg timestamp for 1359 entries:  133 \n",
      "\n",
      "missing timestamp and high rating:  63\n",
      "Missing values after deleting the remaining entries with a rating < 6: 0\n"
     ]
    }
   ],
   "source": [
    "missing_df = ratings_df[ratings_df['Tidstempel'].isnull()]\n",
    "not_missing_df = ratings_df[ratings_df['Tidstempel'].notnull()]\n",
    "\n",
    "# Store the timestamp of the date of conversion (965088000) as a variable\n",
    "date = '01/08/2000 01:00:00'  # UTC +1\n",
    "date_of_conversion = time.mktime(datetime.datetime.strptime(date, \"%d/%m/%Y %H:%M:%S\").utctimetuple())\n",
    "print('date_of_conversion', date_of_conversion)  # 965088000\n",
    "\n",
    "old_scaling = not_missing_df[not_missing_df['Tidstempel'] < date_of_conversion]\n",
    "new_scaling = not_missing_df[not_missing_df['Tidstempel'] >= date_of_conversion]\n",
    "\n",
    "rated_before = old_scaling['BrukerID'].unique().tolist()\n",
    "rated_after = new_scaling['BrukerID'].unique().tolist()\n",
    "rated_before_and_after = []\n",
    "\n",
    "for user in rated_before:\n",
    "    if user in rated_after:\n",
    "        rated_before_and_after.append(user)\n",
    "# print('RATED BOTH:', len(rated_before_and_after))   # 383 user rated both before and after\n",
    "\n",
    "rated_only_before = sorted(list(set(rated_before).difference(rated_before_and_after)))\n",
    "rated_only_after = sorted(list(set(rated_after).difference(rated_before_and_after)))\n",
    "\n",
    "imputed = 0\n",
    "for row in missing_df.iterrows():\n",
    "    index = row[0]\n",
    "    user_id = int(row[1][0])\n",
    "\n",
    "    if user_id in rated_only_before:\n",
    "        avg_timestamp = old_scaling.loc[old_scaling['BrukerID'] == user_id, 'Tidstempel'].mean()\n",
    "        ratings_df.loc[index, 'Tidstempel'] = avg_timestamp\n",
    "        imputed += 1\n",
    "\n",
    "    elif user_id in rated_only_after:\n",
    "        avg_timestamp = new_scaling.loc[new_scaling['BrukerID'] == user_id, 'Tidstempel'].mean()\n",
    "        ratings_df.loc[index, 'Tidstempel'] = avg_timestamp\n",
    "        imputed += 1\n",
    "\n",
    "print(f'Total number of missing values after imputing the avg timestamp for {imputed} entries: ',\n",
    "      ratings_df.isnull().sum().sum(), '\\n')\n",
    "\n",
    "# Now we can determine if there are any of the 131 remaining ratings with a rating > 5:\n",
    "missing_df = ratings_df[ratings_df['Tidstempel'].isnull()]\n",
    "missing_and_high_rating = missing_df.loc[missing_df['Rangering'] > 5]\n",
    "print('missing timestamp and high rating: ', len(missing_and_high_rating))\n",
    "\n",
    "# Imputing the average timestamp for old ratings for the 63 entries with missing timestamp and a rating > 5\n",
    "for row in missing_and_high_rating.iterrows():\n",
    "    index = row[0]\n",
    "    user_id = int(row[1][0])\n",
    "    avg_timestamp = old_scaling.loc[:, 'Tidstempel'].mean()\n",
    "    ratings_df.loc[index, 'Tidstempel'] = avg_timestamp\n",
    "\n",
    "# Dropping remaining 68 entries with missing timestamp values and a rating < 6\n",
    "ratings_df.dropna(how='any', inplace=True)\n",
    "\n",
    "print(f'Missing values after deleting the remaining entries with a rating < 6: {ratings_df.isnull().sum().sum()}')\n",
    "\n",
    "\n",
    "# Converting the scale for ratings prior to 01.08.2000 from 1-10 to 1-5.\n",
    "# First, converting the timestamp from float to int\n",
    "ratings_df['Tidstempel'] = ratings_df['Tidstempel'].astype(int)\n",
    "\n",
    "# Splitting the dataset into pre and post conversion date again,\n",
    "# this time converting the old ratings to the new scale.\n",
    "old_scaling = ratings_df[ratings_df['Tidstempel'] < date_of_conversion]\n",
    "new_scaling = ratings_df[ratings_df['Tidstempel'] >= date_of_conversion]\n",
    "\n",
    "pd.reset_option('mode.chained_assignment')  # THE CODE BELOW CAUSES SETTINGWITHCOPYWARNING, DUE TO CHAINED ASSIGNMENT\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    # Converting the scale for ratings from 1-10 to 1-5.\n",
    "    # Replacing the values from integers to strings, so that I can use regex on them\n",
    "    old_scaling['Rangering'].replace(to_replace=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                                     value=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], inplace=True)\n",
    "\n",
    "    # Using regex to find values and then replacing them with corresponding values on the new scale.\n",
    "    # 1's and 2's = 1, 3's and 4's = 2, etc.\n",
    "    old_scaling['Rangering'].replace(regex=True, to_replace=['1\\b|2', '3|4', '5|6', '7|8', '9|10'],\n",
    "                                     value=[1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "    # Appending the new ratings to the old\n",
    "    ratings_df_cleaned = old_scaling.append(new_scaling, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data folder already exists\n"
     ]
    }
   ],
   "source": [
    "# Creating a new folder for the cleaned data, unless it already exists\n",
    "if path.exists('cleaned_data'):\n",
    "    print('cleaned_data folder already exists')\n",
    "else:\n",
    "    makedirs('cleaned_data')\n",
    "\n",
    "# Writing the dataframe to a .csv file and storing it in cleaned data\n",
    "ratings_df_cleaned.to_csv('cleaned_data/rangering.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "## <a id=\"users\"></a>2) Cleaning 'bruker.json' for user dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrukerID</th>\n",
       "      <th>Kjonn</th>\n",
       "      <th>Alder</th>\n",
       "      <th>Jobb</th>\n",
       "      <th>Postkode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>92103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55405-2546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>44089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>48105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BrukerID Kjonn  Alder  Jobb    Postkode\n",
       "0         0  None   45.0   6.0       92103\n",
       "1         1     M   50.0  16.0  55405-2546\n",
       "2         2     M   18.0  20.0       44089\n",
       "3         3     M    NaN   1.0       33304\n",
       "4         4     M   35.0   6.0       48105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset as DataFrame object, setting column labels\n",
    "users_df = pd.read_json('raw_data/bruker.json', orient='split')\n",
    "\n",
    "users_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrukerID</th>\n",
       "      <th>Alder</th>\n",
       "      <th>Jobb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6040.000000</td>\n",
       "      <td>5046.000000</td>\n",
       "      <td>5447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3020.465894</td>\n",
       "      <td>30.666072</td>\n",
       "      <td>9.104278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1743.799216</td>\n",
       "      <td>12.954723</td>\n",
       "      <td>11.239708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1510.750000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3020.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4530.250000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6040.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BrukerID        Alder         Jobb\n",
       "count  6040.000000  5046.000000  5447.000000\n",
       "mean   3020.465894    30.666072     9.104278\n",
       "std    1743.799216    12.954723    11.239708\n",
       "min       0.000000     1.000000     0.000000\n",
       "25%    1510.750000    25.000000     3.000000\n",
       "50%    3020.500000    25.000000     7.000000\n",
       "75%    4530.250000    35.000000    14.000000\n",
       "max    6040.000000    56.000000    99.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe dataset\n",
    "users_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6040 rows in the dataset.\n",
      "Proportion of missing data for each column in %: \n",
      "BrukerID     0.00\n",
      "Kjonn        5.02\n",
      "Alder       16.46\n",
      "Jobb         9.82\n",
      "Postkode     7.47\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_vals = users_df.isnull().sum()\n",
    "perc = round(missing_vals / users_df.shape[0] * 100, 2)\n",
    "print(f'There are {users_df.shape[0]} rows in the dataset.')\n",
    "print(f'Proportion of missing data for each column in %: \\n{perc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.5% of the userbase has not recorded their age. This is a significant proportion, so I will have to be careful when handling these missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 291 rows with at least 2 missing values\n",
      "There are 20 rows with at least 3 missing values\n"
     ]
    }
   ],
   "source": [
    "# Count rows that have at least N missing values\n",
    "def count_rows_with_n_missing_vals(dataframe, n):\n",
    "    missing_val_count = dataframe.shape[0] - (dataframe.dropna(how='any', thresh=(dataframe.shape[1] - n)+1).shape[0])\n",
    "    return missing_val_count\n",
    "\n",
    "\n",
    "print(f'There are {count_rows_with_n_missing_vals(users_df, 2)} rows with at least 2 missing values')\n",
    "print(f'There are {count_rows_with_n_missing_vals(users_df, 3)} rows with at least 3 missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few records (20) that have most of the data missing, and there are almost 300 records that are missing 2 values or more. However, since these users are likely to have recommended movies and that their recommendations are recorded in the ratings dataset, I can't just remove these values altogether. I am also restricted by the fact that training a model with a dataset that has a lot of missing values might significantly impact its accuracy. User-based filtering algorithms assume that all values are numerical and that they hold a meaningful value.\n",
    "\n",
    "I can conclude that these missing values need to be filled, but I should be wary of just imputing a value of 0 or an average or median value. The features in the user dataset are somewhat correlated, and this correlation is important for my recommendation system to utilize. Luckily, there are some features that are more suited for constant imputation than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation for missing data in users dataset: Constant\n",
    "\n",
    "Imputing a 0 to fill missing values in the 'Jobb' -column is accurate, as this label indicates 'other/unspecified' according to the README for this dataset. Also, since we are dealing with a continuous variable that is not related to any other independent variable, imputing a constant value shouldn't lead to a significant loss inefficiency. Executing this step before doing any further imputations might benefit a predictive modeling approach to imputing the rest of the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrukerID</th>\n",
       "      <th>Kjonn</th>\n",
       "      <th>Alder</th>\n",
       "      <th>Jobb</th>\n",
       "      <th>Postkode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>92103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55405-2546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>44089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>48105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BrukerID Kjonn  Alder  Jobb    Postkode\n",
       "0         0  None   45.0   6.0       92103\n",
       "1         1     M   50.0  16.0  55405-2546\n",
       "2         2     M   18.0  20.0       44089\n",
       "3         3     M    NaN   1.0       33304\n",
       "4         4     M   35.0   6.0       48105"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill NA values in 'Jobb' column with 0, indicating 'other/unspecified'\n",
    "users_df['Jobb'].fillna(0, inplace=True)\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation for missing data in users dataset: KNN\n",
    "\n",
    "The remainding columns with missing values are 'Postkode' (postal code/zip code), 'Kjonn' (gender) and 'Alder'(age). For these features I will use KNN imputation to handle the missing data. The KNN imputation algorithm will find predict a new sample to replace the missing value in the target row by finding the N (5 in this case) most similar rows to the target and then calculating the average of these. These similar rows don't have any missing data (at least not for the target feature), so the predicted value can then be inserted into the target row. \n",
    "\n",
    "Using KNN will be more accurate than taking a simple average, but though age, gender, field of work and address may be somewhat related, the correlation is not likely to be very strong. However, KNN algorihms are very sensitive to outliers, so the spread of the dataset might increase, which might make it easier to make recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR KNN IMPUTER\n",
    "# 1) Normalizing gender categories from 'M' and 'F' to 1 and 0\n",
    "users_df.replace(to_replace=['M', 'F'], value=[1, 0], inplace=True)\n",
    "\n",
    "# 2) Adding the postal codes without the hyphen and the following integers with Sector, Blocks, and Side of Street\n",
    "#    from postal codes to a new column, which can be used for KNN predictions\n",
    "users_df['Postkode_5'] = users_df.Postkode.str[:3]\n",
    "\n",
    "# 3) Removing features that are unecessary/unhelpful for KNN predictions\n",
    "df = users_df.drop(['Postkode', 'BrukerID'], axis=1)\n",
    "\n",
    "# 4) Scaling the numeric variables to have values between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kjonn         0\n",
      "Alder         0\n",
      "Jobb          0\n",
      "Postkode_5    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Perform KNN imputation on missing values\n",
    "imputer = KNNImputer(n_neighbors=5, )\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BrukerID Kjonn  Alder  Jobb    Postkode\n",
      "0          0     F     45     6       92103\n",
      "1          1     M     50    16  55405-2546\n",
      "2          2     M     18    20       44089\n",
      "3          3     M     35     1       33304\n",
      "4          4     M     35     6       48105\n",
      "5          5     M     25    20         664\n",
      "6          6     M     50    14         379\n",
      "7          7     F     25     0         264\n",
      "8          8     M     25     4       70806\n",
      "9          9     M     25    19       45701\n",
      "10        10     F     18     1       95864\n",
      "11        11     M     35     1         478\n",
      "12        12     M     45     0       10543\n",
      "13        13     M     50     7       34243\n",
      "14        14     M     25     4       53140\n",
      "15        15     F     18     4       60625\n",
      "16        16     M     25    17       03570\n",
      "17        17     M     35     7       30117\n",
      "18        18     M     50     1       01096\n",
      "19        19     M     25    15       02143\n"
     ]
    }
   ],
   "source": [
    "# Reverting scaling, so that the data is readable\n",
    "df = pd.DataFrame(scaler.inverse_transform(df), columns=df.columns)\n",
    "\n",
    "# Adding 'BrukerID' and complete postal codes back again\n",
    "df[['BrukerID', 'Postkode']] = users_df[['BrukerID', 'Postkode']]\n",
    "\n",
    "# Renaming the new dataframe in order to replace the old one, while reordering the columns simultaenously\n",
    "users_df = df[['BrukerID', 'Kjonn', 'Alder', 'Jobb', 'Postkode', 'Postkode_5']]\n",
    "\n",
    "# Iterating over the dataframe, fixing stuff as I go\n",
    "for row in users_df.iterrows():\n",
    "    index = row[0]\n",
    "    gender = row[1][1]\n",
    "    age = row[1][2]\n",
    "    job = row[1][3]\n",
    "    postcode = str(row[1][4])\n",
    "    postcode_5 = row[1][5]\n",
    "\n",
    "    # Rounding imputed postal code values from float to whole int. Then convert to string\n",
    "    users_df.iloc[index, 5] = str(int(round(postcode_5)))\n",
    "\n",
    "    # Inserting the full postal codes back into the new postal code column.\n",
    "    # This leaves the imputed postcode values as three-digit postal codes,\n",
    "    # which means that they can't be used to send anything by mail. However, they can be used for\n",
    "    # user-based filtering or profiling, as a postal code with three digits indicates the region/city\n",
    "    # in which the user most likely would reside in.\n",
    "    if len(postcode) >= 5:\n",
    "        users_df.iloc[index, 5] = postcode\n",
    "\n",
    "    # Rounding/normalizing gender values from float to either 'M' if >=0.5 or else 'F'\n",
    "    if gender >= 0.5:\n",
    "        users_df.iloc[index, 1] = 'M'\n",
    "    else:\n",
    "        users_df.iloc[index, 1] = 'F'\n",
    "\n",
    "    # Rounding age values to closest age interval according to the README\n",
    "    if age < 18:\n",
    "        users_df.iloc[index, 2] = 1\n",
    "    elif 18 <= age < 25:\n",
    "        users_df.iloc[index, 2] = 18\n",
    "    elif 25 <= age < 35:\n",
    "        users_df.iloc[index, 2] = 25\n",
    "    elif 35 <= age < 45:\n",
    "        users_df.iloc[index, 2] = 35\n",
    "    elif 45 <= age < 50:\n",
    "        users_df.iloc[index, 2] = 45\n",
    "    elif 50 <= age < 56:\n",
    "        users_df.iloc[index, 2] = 50\n",
    "    else:\n",
    "        users_df.iloc[index, 2] = 56\n",
    "\n",
    "# Changing features that are dtype float (Kjonn, Alder, Jobb) back into dtype int\n",
    "users_df[['Alder', 'Jobb']] = users_df[['Alder', 'Jobb']].astype(dtype=int)\n",
    "\n",
    "# Remove the old postal code column, rename the new one\n",
    "users_df['Postkode'] = users_df['Postkode_5']\n",
    "users_df.drop('Postkode_5', axis=1, inplace=True)\n",
    "\n",
    "print(users_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6040 rows in the dataset.\n",
      "Proportion of missing data for each column in %: \n",
      "BrukerID    0.0\n",
      "Kjonn       0.0\n",
      "Alder       0.0\n",
      "Jobb        0.0\n",
      "Postkode    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values again\n",
    "missing_vals = users_df.isnull().sum()\n",
    "perc = round(missing_vals / users_df.shape[0] * 100, 2)\n",
    "print(f'There are {users_df.shape[0]} rows in the dataset.')\n",
    "print(f'Proportion of missing data for each column in %: \\n{perc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the dataframe to a .csv file and storing it in cleaned data folder\n",
    "users_df.to_csv('cleaned_data/bruker.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "## <a id=\"movies\"></a>3) Cleaning 'film.xlsx': Preparing the movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3883 movies in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Load data from Excel file to DataFrame object\n",
    "excel = pd.ExcelFile('raw_data/film.xlsx')\n",
    "movies_df = excel.parse(sheet_name='film', index_col=None)\n",
    "movies_df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "print(f'There are {movies_df.count()[0]} movies in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of missing data for each column in %: \n",
      "FilmID     0.0\n",
      "Tittel     0.0\n",
      "Sjanger    0.0\n",
      "dtype: float64\n",
      "Empty DataFrame\n",
      "Columns: [FilmID, Tittel, Sjanger]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [FilmID, Tittel, Sjanger]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_vals = movies_df.isnull().sum()\n",
    "perc = round(missing_vals / movies_df.shape[0] * 100, 2)\n",
    "print(f'Proportion of missing data for each column in %: \\n{perc}')\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = movies_df[movies_df.duplicated(['FilmID'])]\n",
    "print(duplicate_rows)\n",
    "duplicate_rows = movies_df[movies_df.duplicated(['Tittel'])]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing or duplicate values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONVERTING GENRE FEATURE FROM ONE NOMINAL COLUMN TO 18 DISCRETE COLUMNS\n",
    "# 1) Adding labels for the 18 different genres with 0 as default value\n",
    "genres = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\",\n",
    "          \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "for genre in genres:\n",
    "    movies_df.loc[:,genre] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Looping over the nominal genre column, and setting the discrete columns accordingly\n",
    "for row in movies_df.iterrows():\n",
    "    index = row[0]\n",
    "    genre_data = row[1][2]\n",
    "\n",
    "    genres = movies_df.columns.values.tolist()\n",
    "    genres.remove('FilmID')\n",
    "    genres.remove('Tittel')\n",
    "    genres.remove('Sjanger')\n",
    "\n",
    "    for genre in genres:\n",
    "        if genre in genre_data:\n",
    "            movies_df.loc[index, genre] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove genre nominal column, as it is no longer needed\n",
    "movies_df.drop(labels=['Sjanger'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the dataframe to a .csv file and storing it in cleaned data folder\n",
    "movies_df.to_csv('cleaned_data/film.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
